{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e673a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated test annotations: data/test_annotations.csv with 13320 entries.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "dataset_dir = \"data/UCF101\"\n",
    "output_csv = \"data/test_annotations.csv\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    for video_file in os.listdir(class_path):\n",
    "        if video_file.endswith(\".avi\"):\n",
    "            video_path = os.path.join(class_name, video_file)\n",
    "            rows.append([video_path, class_name])\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_csv, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"label\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\" Generated test annotations: {output_csv} with {len(rows)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169dedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1665/1665 [27:47<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report saved at: results/classification_report.json\n",
      "Confusion matrix saved at: results/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from config import Config\n",
    "from data_loader import VideoDataset\n",
    "from model import CNN_LSTM\n",
    "from transforms import get_transforms\n",
    "\n",
    "# Load model from checkpoint\n",
    "def load_model(checkpoint_path, device):\n",
    "    model = CNN_LSTM(\n",
    "        num_classes=Config.num_classes,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        hidden_dim=Config.hidden_dim,\n",
    "        lstm_layers=Config.lstm_layers,\n",
    "        dropout=Config.dropout\n",
    "    )\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the label map from the classInd.txt file\n",
    "def load_label_map(path):\n",
    "    label_map = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            index, label = line.strip().split()\n",
    "            label_map[int(index) - 1] = label  # Convert to 0-based indexing\n",
    "    return label_map\n",
    "\n",
    "def test():\n",
    "    # Load the model and label map\n",
    "    model = load_model(Config.model_save_path, Config.device)\n",
    "    label_map = load_label_map(Config.label_map_path)\n",
    "\n",
    "    # Initialize dataset and dataloader for testing\n",
    "    transform = get_transforms(Config.img_size)\n",
    "    test_dataset = VideoDataset(\n",
    "        dataset_path=Config.dataset_path,\n",
    "        label_map_path=Config.label_map_path,\n",
    "        cache_path=Config.cache_path,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        img_size=Config.img_size,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_dataset.test_annotation_path = Config.test_annotation_path  # Set the test annotation path\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers\n",
    "    )\n",
    "\n",
    "    # Prepare for evaluation\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs = inputs.to(Config.device)\n",
    "            labels = labels.to(Config.device)\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    unique_classes = sorted(set(y_true) | set(y_pred))\n",
    "    target_names = [label_map[i] for i in unique_classes]\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=target_names,\n",
    "        labels=unique_classes,\n",
    "        output_dict=True\n",
    "    )\n",
    "    os.makedirs(Config.results_dir, exist_ok=True)\n",
    "    report_path = os.path.join(Config.results_dir, \"classification_report.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    print(\"Classification report saved at:\", report_path)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    cm_path = os.path.join(Config.results_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix saved at:\", cm_path)\n",
    "\n",
    "# Run the test\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39214254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from config import Config\n",
    "from model import CNN_LSTM\n",
    "from transforms import get_transforms\n",
    "\n",
    "def load_model():\n",
    "    model = CNN_LSTM(\n",
    "        num_classes=Config.num_classes,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        hidden_dim=Config.hidden_dim,\n",
    "        lstm_layers=Config.lstm_layers,\n",
    "        dropout=Config.dropout\n",
    "    ).to(Config.device)\n",
    "\n",
    "    checkpoint = torch.load(Config.model_save_path, map_location=Config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_video(video_path, transform):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    if total_frames < Config.frames_per_clip:\n",
    "        frames += [frames[-1]] * (Config.frames_per_clip - total_frames)\n",
    "    else:\n",
    "        step = total_frames // Config.frames_per_clip\n",
    "        frames = frames[::step][:Config.frames_per_clip]\n",
    "\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    frames = torch.stack(frames, dim=0)  # (sequence_length, 3, img_size, img_size)\n",
    "    return frames\n",
    "\n",
    "def predict_multiple_videos(video_paths):\n",
    "    model = load_model()\n",
    "    transform = get_transforms(Config.img_size)\n",
    "\n",
    "    batch = []\n",
    "    for video_path in video_paths:\n",
    "        frames = preprocess_video(video_path, transform)\n",
    "        batch.append(frames)\n",
    "\n",
    "    batch = torch.stack(batch, dim=0)  # (batch_size, sequence_length, 3, img_size, img_size)\n",
    "    batch = batch.to(Config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "    predicted_classes = [Config.class_names[pred.item()] for pred in preds]\n",
    "\n",
    "    for path, pred_class in zip(video_paths, predicted_classes):\n",
    "        print(f\"{path} ➔ Predicted Class: {pred_class}\")\n",
    "\n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3344f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCF101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ➔ Predicted Class: ApplyEyeMakeup\n",
      "data/UCF101/Archery/v_Archery_g01_c01.avi ➔ Predicted Class: Archery\n",
      "data/UCF101/PlayingPiano/v_PlayingPiano_g01_c01.avi ➔ Predicted Class: PlayingPiano\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ApplyEyeMakeup', 'Archery', 'PlayingPiano']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list = [\n",
    "    \"data/UCF101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\",\n",
    "    \"data/UCF101/Archery/v_Archery_g01_c01.avi\",\n",
    "    \"data/UCF101/PlayingPiano/v_PlayingPiano_g01_c01.avi\"\n",
    "]\n",
    "\n",
    "predict_multiple_videos(video_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "994f6a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCF101/Surfing/v_Surfing_g01_c01.avi ➔ Predicted Class: Surfing\n",
      "data/UCF101/WritingOnBoard/v_WritingOnBoard_g01_c01.avi ➔ Predicted Class: WritingOnBoard\n",
      "data/UCF101/YoYo/v_YoYo_g25_c04.avi ➔ Predicted Class: YoYo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Surfing', 'WritingOnBoard', 'YoYo']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list = [\n",
    "    \"data/UCF101/Surfing/v_Surfing_g01_c01.avi\",\n",
    "    \"data/UCF101/WritingOnBoard/v_WritingOnBoard_g01_c01.avi\",\n",
    "    \"data/UCF101/YoYo/v_YoYo_g25_c04.avi\"\n",
    "]\n",
    "\n",
    "predict_multiple_videos(video_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ecef8",
   "metadata": {},
   "source": [
    "New model fine_tuning , checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39d78d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1665/1665 [27:08<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report saved at: results/classification_report.json\n",
      "Confusion matrix saved at: results/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from config import Config\n",
    "from data_loader import VideoDataset\n",
    "from model import CNN_LSTM\n",
    "from transforms import get_transforms\n",
    "\n",
    "# Load model from checkpoint\n",
    "def load_model(checkpoint_path, device):\n",
    "    model = CNN_LSTM(\n",
    "        num_classes=Config.num_classes,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        hidden_dim=Config.hidden_dim,\n",
    "        lstm_layers=Config.lstm_layers,\n",
    "        dropout=Config.dropout\n",
    "    )\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the label map from the classInd.txt file\n",
    "def load_label_map(path):\n",
    "    label_map = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            index, label = line.strip().split()\n",
    "            label_map[int(index) - 1] = label  # Convert to 0-based indexing\n",
    "    return label_map\n",
    "\n",
    "def test():\n",
    "    # Load the model and label map\n",
    "    model = load_model(Config.model_save_path, Config.device)\n",
    "    label_map = load_label_map(Config.label_map_path)\n",
    "\n",
    "    # Initialize dataset and dataloader for testing\n",
    "    transform = get_transforms(Config.img_size)\n",
    "    test_dataset = VideoDataset(\n",
    "        dataset_path=Config.dataset_path,\n",
    "        label_map_path=Config.label_map_path,\n",
    "        cache_path=Config.cache_path,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        img_size=Config.img_size,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_dataset.test_annotation_path = Config.test_annotation_path  # Set the test annotation path\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers\n",
    "    )\n",
    "\n",
    "    # Prepare for evaluation\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs = inputs.to(Config.device)\n",
    "            labels = labels.to(Config.device)\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    unique_classes = sorted(set(y_true) | set(y_pred))\n",
    "    target_names = [label_map[i] for i in unique_classes]\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=target_names,\n",
    "        labels=unique_classes,\n",
    "        output_dict=True\n",
    "    )\n",
    "    os.makedirs(Config.results_dir, exist_ok=True)\n",
    "    report_path = os.path.join(Config.results_dir, \"classification_report.json\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    print(\"Classification report saved at:\", report_path)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    cm_path = os.path.join(Config.results_dir, \"confusion_matrix.png\")\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix saved at:\", cm_path)\n",
    "\n",
    "# Run the test\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bdd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from config import Config\n",
    "from model import CNN_LSTM\n",
    "from transforms import get_transforms\n",
    "\n",
    "def load_model():\n",
    "    model = CNN_LSTM(\n",
    "        num_classes=Config.num_classes,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        hidden_dim=Config.hidden_dim,\n",
    "        lstm_layers=Config.lstm_layers,\n",
    "        dropout=Config.dropout\n",
    "    ).to(Config.device)\n",
    "\n",
    "    checkpoint = torch.load(Config.model_save_path, map_location=Config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_video(video_path, transform):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    if total_frames < Config.frames_per_clip:\n",
    "        frames += [frames[-1]] * (Config.frames_per_clip - total_frames)\n",
    "    else:\n",
    "        step = total_frames // Config.frames_per_clip\n",
    "        frames = frames[::step][:Config.frames_per_clip]\n",
    "\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    frames = torch.stack(frames, dim=0)  # (sequence_length, 3, img_size, img_size)\n",
    "    return frames\n",
    "\n",
    "def predict_multiple_videos(video_paths):\n",
    "    model = load_model()\n",
    "    transform = get_transforms(Config.img_size)\n",
    "\n",
    "    batch = []\n",
    "    for video_path in video_paths:\n",
    "        frames = preprocess_video(video_path, transform)\n",
    "        batch.append(frames)\n",
    "\n",
    "    batch = torch.stack(batch, dim=0)  # (batch_size, sequence_length, 3, img_size, img_size)\n",
    "    batch = batch.to(Config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "    predicted_classes = [Config.class_names[pred.item()] for pred in preds]\n",
    "\n",
    "    for path, pred_class in zip(video_paths, predicted_classes):\n",
    "        print(f\"{path} ➔ Predicted Class: {pred_class}\")\n",
    "\n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccbd410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCF101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi ➔ Predicted Class: ApplyEyeMakeup\n",
      "data/UCF101/Archery/v_Archery_g01_c01.avi ➔ Predicted Class: Archery\n",
      "data/UCF101/PlayingPiano/v_PlayingPiano_g01_c01.avi ➔ Predicted Class: PlayingPiano\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ApplyEyeMakeup', 'Archery', 'PlayingPiano']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_list = [\n",
    "    \"data/UCF101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\",\n",
    "    \"data/UCF101/Archery/v_Archery_g01_c01.avi\",\n",
    "    \"data/UCF101/PlayingPiano/v_PlayingPiano_g01_c01.avi\"\n",
    "]\n",
    "\n",
    "predict_multiple_videos(video_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81734dab",
   "metadata": {},
   "source": [
    "display first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a164a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.117904..2.64].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: data/UCF101/Basketball/v_Basketball_g01_c01.avi\n",
      "Predicted Class: Basketball\n",
      "\n",
      " Displaying first frame from the video:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAHiCAYAAADf3nSgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKHFJREFUeJzt3XuUZWdZJ+BdUA2psKpJN9CRtCYMnSER6IFGghAXcRGGywJ0ZZRbFBBlELMYjcPg6CiC4g0FGSECI2GQSxbCYFQ0MoATAokiEiAwCdhimkUD3dLN2B26TKpDFan5w6Ur32/v1HdOXfo7Vf08/73n7Mt39jm13979fpeppaWlpQ4AOOHu1roBAHCykoQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAamR51w6mpqfVsB8Cq7H70K4v44c+7uIjPPOvsIt6yOF/E2+b7t8NH7t5SxI95cPn+uHfFN156oIj/+1VvLeK/3/eLYx6x7z/8ZRn/yWPdu1sZZUJKT8IA0IgkDACNSMIA0MjUqKsoqQkDLC9vp9dHffa6d91QxJe/+Z1F/JLf+rXeMV/4kplVtWnq6VeVL1z55tjiz1Z1fO6amjAATDBJGAAakYQBoBE1YYANZOsDy/HQv3tZObb4uU9Z3fGvH3gtq9JnRnxlxD8mX3RdpyYMABNNEgaARiRhAGhETRhgExnxlr6mfvzX9hfx5S97wAlvwyRSEwaACSYJA0AjkjAANKImDLCJtKgJL32hjO92jnzRdWrCADDRJGEAaEQSBoBGJGEAaETHLICT2oMjnh3YZrpyjL1FtKf7sSK+oXv12K3aDHTMAoAJJgkDQCOSMAA0oiYMwEnoEUW0tPSpIj4ysMd8xDsjPhTx6SO0wpMwADQiCQNAI5IwADRSG/wFAJvOr374U8u+v30FxxylBpw8CQNAI5IwADQiCQNAI8YJA3ASuncRPfWXPlfEV70iRwGvD0/CANCIJAwAjUjCANCImjAAjOQ+ZXj3RxXh7gsuLOL/++GXVo/oSRgAGpGEAaARSRgAGjF3NACM5B/L8Fv/O96/sBuXJ2EAaEQSBoBGJGEAaMQ4YQBYB6OkV0/CANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUjCANCIJAwAjUy3bsDkOTXi25q0AoDNz5MwADQiCQNAI5IwADSiJtzz0ohfOeb+94j4m6toCwCbmSdhAGhEEgaARiRhAGhEEgaARqaWlpaWRtpwamq92zIhvjvix0f8BxFfGPHuiK+N+I9GaMO3Rfy1EfYBYJKMkl49CQNAI5IwADQiCQNAI2rCj/2ZMr7uzbHBNyoHeGbEhyP+yNhNAmDjUxMGgAkmCQNAI5IwADRiAYcb95Xx4365jK/56djhPhEvRvyRVTcJgJODJ2EAaEQSBoBGJGEAaERN+JaYy3nm4WX8kBeU8ef2xwEOrXWLADhJeBIGgEYkYQBoRBIGgEbUhNP80TK+4KIy3h/rCf/Tu9a1OQBsXp6EAaARSRgAGpGEAaAR6wnXPDDWG/7iq9u0A4ANxXrCADDBJGEAaEQSBoBG1IQBYB2oCQPABJOEAaARSRgAGpGEAaARSRgAGpGEAaARSRgAGpGEAaARSRgAGpGEAaARSRgAGpGEAaCR6dYNYLMq/3039bCfK+Klz35gYJ9Pr2N7ACaPJ2EAaEQSBoBGJGEAaGRqaZRVh7uum5qaWu+2MEH+Kn4W3xPf/1VfL99/6n3L/RfieFsi3jtwzu+cely88pHlmggw0UZJr56EAaARSRgAGpGEAaAR44Tpuq7rtj/n5cu+nzXiz32+fP/3j5Tx7PYy3hU14z2DZ/nIsm0A2Gw8CQNAI5IwADQiCQNAI2rCdF3XdRc+/vwiPj/ef8ZlZfzI88r4rJ1l/OUDZfzrv3JzEX/6dWf32vC6P/xqEV/69G8fbCvAZuFJGAAakYQBoBFJGAAaUROm67quW1ycX/b9XWeV8U8+uoxPzR2+owz3ndevAacnPm1ndRuAzcSTMAA0IgkDQCOSMAA0Yj1hBuXP4tK3le/PzEQcvQtmt5bxlX9cTi79V2+MyaW7rjt0Rxl/29395oCNy3rCADDBJGEAaEQSBoBGjBNm0Ps+VcbP2V3Gl/9MOa748mt+sIh33/NlRfzyt5SzUS8MnPMJP/yKsdoIsNF5EgaARiRhAGhEEgaARiRhAGjEZB2s0I9EvD2i2SI+0r0htv/HtW8SE+QFRXT1vrcU8UMeWG79uS/2j/BzL7mqiK9/3/etScvgRDFZBwBMMEkYABqRhAGgETVhYPUe+Moi/Nt9v1jEH/7LcvPj5Vwv3f3663l0Z51Vxt97P/cgNhY1YQCYYJIwADQiCQNAI2rCwKrdGreR98S43xc863VF/JgLLiji6S39tWRmMz50sIjf/bYnj9dIOMHUhAFggknCANCIJAwAjagJAytQ/vt9aelbRXztLeXW208r44fG0RYGzvBbbyvjX3h+GU9N3T/2+NrAUaAdNWEAmGCSMAA0IgkDQCP9wXkAPfcuoqWlW4r4wK3l1oc/WMZHo+h7Qznkt/ujd/TP+Mc3lfEDznlJbKEGzMbnSRgAGpGEAaARSRgAGlETBkbwjSK69LL9Rfz8i8vFfw/PlXu/+IU/HMd7WhHtOXNP74zv/IvymDMPmCk3+MJdNBU2EE/CANCIJAwAjUjCANCIuaOBtXfPnyjj2/cW4ZbuqUW80MWg4K7ruu7tEd874m90MMnMHQ0AE0wSBoBGJGEAaERNGADWgZowAEwwSRgAGpGEAaARSRgAGrGAw7r7QMSvGNjmb05EQwCYMJ6EAaARSRgAGpGEAaARNeFV+3cRb4/4ySeqIQBsMJ6EAaARSRgAGpGEAaARCzgwovz32h1NWlG6R+X9mYhPxCLwp0Z82wk4JzCJLOAAABNMEgaARiRhAGhETZgNZBLr0qn279pJbDOwHtSEAWCCScIA0IgkDACNmDuaDWQj1FM3QhuBSeFJGAAakYQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAaMXc0wAnzQ0W0vbuoiI90vz2wz9+sX3NozpMwADQiCQNAI5IwADQiCQNAIzpmAZwwh4voSHdTEW/prujtsdD923VtEW15EgaARiRhAGhEEgaARqaWlpaWRtpwamq92wJwkvm2Mrz7Z3tbvPCyHUV8+TveW27w8WeudaNYI6OkV0/CANCIJAwAjUjCANCImjBAM/+miH70Z7/Y2+Ktryrj98X7F+364fKFL75rDdrFWlATBoAJJgkDQCOSMAA0Yu5ogGbKMcAzVw1sEjXhM/P98y8uYzXhDcWTMAA0IgkDQCOSMAA0srHHCZ8W8dZTy3h2axkvDBzjC19bwwYBjONHIn5Sb4ulpYt7r93Z1HkHyhc++e2rbBNrxThhAJhgkjAANCIJA0AjG2qc8PanfE8RL27ZUsTT0/Fx5ufLOLbvuq47PlPGt31WjfifRX29u61JK2BzeXnEZ0e8vXqER1wUL6gBb2iehAGgEUkYABqRhAGgkQ1VE3720x5fxDdFyXc2Sr5nRI14+45+veWKP/lwEd929APlBscWy/iWb9YbuimoAcPau6Dy/mL/pdvLcN+Na9YYJoAnYQBoRBIGgEYkYQBoZEPVhHftKMfUTS+WReH5mBx6545yrc4PX/+Z3jEPHDkSJ8lxe7NleM1fVdu5avcvx/2duuf8Ij4nPlcMde6mo6y02JXX6cydO3unfPdvvrKItz7uB4r42DVXxx7f6B2jdI/K+0NOlno7J693Rrw/4iv6u3ysDJ8ft6jXf3G1baIlT8IA0IgkDACNSMIA0IgkDACNTC2Nsupw13VTU1Pr3Zaeq7/86SKeLvtddQejY1Z60wevLV/YNtvbZv/BfWV89UeL+LwLnlzE17/s15c951o46zn/qYifGpOUPDI7VuVlmI8LNV92PhvqjXfdzXvLNuwse38cmSsXDj8+X/b+mj9WNmJ2sWzDKD0AX/s7by7iLacvP5l9LseR3+6OOOkpucDHwPGvv+7/LHtOWJ1cbOGrEfd/f697YPn3311Uhpe+9vmxx9vHbxbrYpT06kkYABqRhAGgEUkYABqZ6JrwR6Np7//8zUW8a/asIn5KrOCwbV9Zl1w8I6uIXfeLe/+giOcPl7XOy5/9vGXb+OxfeFcRf/lAOfj+Y2/7b8vu33Vd99w3/l4RP/rchxbxjq1Ru4z665bFgUnf72Rh4VgR5+QdXdd1c4fL17ZF3XlmppwSZP7YXHmAuLRbjkVdegSf2VvWpae39L+vsk3lObbF+7PT5f4zW8qq8S+/4629Y85t31rEB+P9Y+/7o2XbdPLIf7/f0aQVG893R/w3ET+zt8fZ3WVFfMmvlpP1/JfX3FDucMsjVtg21pqaMABMMEkYABqRhAGgkYmuCf/S18umPTpKo0/Kf0LE4tddlC2X7ts/xyfvKMfQPuru91m2TZ/YV7Zp7/6yBvy8Cx+w7P7fGLjc7/tsWQvdMVvWLmei4Do9l+OAo8Zblox6Kzx8+Ya9XZrZWtZCt832x1QXcpGIhVxGYnnzh4/0XpueXr4GnJ9jZkuMTV5YfjTyYvwebjpwuLfN3FllP4Pc4j2veU00qWzUbIw9nj2lfH96pmzjn7/p5XfVXDal3ymiU7tLi/i27iW9PS6632uL+NG/XL5/ZSzwcP0VJ/5ezTA1YQCYYJIwADQiCQNAI6NM6dvM7H3L+Yqf1PUXoy/csxIP2HVo+fmJ898p551evnv4+uX3PvuJ31dtww8+7Nwizspor1Kate8RPmfNzEx5lh2nVWrCd5S1zrn58WrCR/Ye6L22fTbGIufw54XyhTNi7PLX9x4q4lMORBF4ppwPe8fu/s9/+47ymIvXll/wz//QTxfx7PTy12loTHbRpIG3Z7qjy+6TFo4vf46ad7/7f4y/0z0fXMa3f37MA5wa8W3jt2FDKvtCnPGQ8t2Dnyvrv13XdR+Or3chulM8MaYyuP6KlbaNFjwJA0AjkjAANCIJA0AjEz1OOMfUbr2L7dbS1I4nFPELX/pzRfzm/1qu7VnOytx1947rdPXBcosL71+ptZ7EsiqYVebeL/AfIr4p4kMRR4n4yEAXg+kYY/2ZGAc+s1AW6KajW8VizuNd6XWx9/hc77XFY/mrqshTVsZbL954YxFft6/SsaHrul17yn4LsVx0d2Df/ng/Nti+p2xj9HS4/MqfHDjrN6vtWt64zxgnYP7ru/9ZGZ/3pCLcuq//3S3GH8I5P17GP/iiMn7Z/YwTnhTGCQPABJOEAaARSRgAGpnomvCN0bRcL7YyarjLStd5A9tkHfIn3lZOxPr8559fxBfG9rlq7j0ueFYRf+3a9xRxDDNmNd4fcU70nEORc9zx0A/olDjE4bJmO38kC81xiqgJzy/0a77F+3vyV911x2eW7/2wMB/rZPfWk15+LefpG8v1Z48v5qrJfWfsLOfUPhjrZs/HGtcLUdE/uvuC8oDR5ps+GhMgd103d6D8AhePlgNkp2PO9MUd5V/X/Pbx+l/8+XWvGmGr7Knwjcr2OR76URF/pAzv9/f9Q8zGXO9H4s52yysrbaAVNWEAmGCSMAA0IgkDQCMTXRP+22jauXex3b/ISlgO0dw3sM8nv1LWlS4+s6zhnPrYHyjiW6+9sogvveKDRfz65z65iM+65EeK+EtvfNtAK1iRnEP7dZXtz4/4owPbZMlvddMy98Xxbl7sr+28o7cg9PLmFsZr5M4tZTH8gwfe0NtmS/zxbDu93Gf+WFnrPpr12m3lnOxzux9eHnD+eLl/Djzuum76SHmO6fycUa9fnC7HV88tLn9d5qOm/P6b63N2z8yUdea//vjNRXysuzb2+Gr1mGxeasIAMMEkYQBoRBIGgEYmej3hbFwO+8yKT5bzchjoUMXntb/3imXbcEqU53KU6PyWWNwz7H/T28sX1ITXTq6jvCfi/GryB/LSgWNmifbqShvGW0a55+xP1Ho6DCjLqfU51XdFHIPVnzT94vHbkMtwZ5zXJYcyxx/vH/ZqqV23ZTrqxDEf9fR0eZD5o2UjZo8sf3vLMvRjtvfbkBYWyzr0Gc95RhH//hXvrh5jzZ3574vw1JiXezZq313E03FdD9z+6bVrG1WehAGgEUkYABqRhAGgEUkYABqZqI5ZObA5hzmvdrqQoQUctuzMHiWlC55RLrp9TfQoufBZFxXxxw/8fBGfsrNcESA7l3VdfSEKRvSEiL8QcXYOyo5dXdf15snI+fbTuH9B2Ybl13dYG7XOY2uxqkhtvpBjEcd1ePpCLPAwsE3vHLV1K2rfTWw/dyBXAOk7Zaa8X3xuPv9676geY609+7lPK+KZuC4zOcnJfH7wsqffh27s/yBmtld+RHHO6enx/jDmj5Rt3PvZPxtr/43MkzAANCIJA0AjkjAANDJRCziM2JSJlnM9nBFxdWIF1s8tEZ82sM1XIl5+LpbxZV1zrY8/yjnTwTU4x2zEWVuv1XdvHDjmUAeKO8sSbpYta7XwUdqQynkwupuiJLz7Qyd+oZvnXvK7RXzK0fKDLS7ENEWH40cXNeLZc8fvJLCYdeYtw9v96ynj2h+MJu0/1l/YpHfOxfJHtdBfA6SwdUv5gzhlZvm69fWf/F/VNvSVz7VLS98acw8A4ISRhAGgEUkYABqZqHHCm8EKpuPnRDkt4lsHtslh47V66riyVpa10vWQHRGydjZUO83X8jpknOfIGnGq1Yi7rj+APrfJmnGtvp5trH3GETw0vs9P373s13JNlAQvj14ju+IH9+cjDdr+7iJ69NEYYz1fjvs9+qUvLft+d7wcxH3Twr7+Kbdm75bS4pFywPvc/PID4Kfj4m9b3FZucNayu3dd13WHj0SngEodemZ7+aOcnV2+08B5j3xmvRG9k4y/iydhAGhEEgaARiRhAGhETZiT170GXrs94qxNrbaGm39xtdrpWpwjZbnu5oFtajXfWp25Jts4NIV7HjOvfX43lZpg73j1qaL78vu6qQz3fCTi+FwvuTp6jfxUGd6ne3XvlOfFB337455RxKfntczvbiZmzc/rFPGHj/bruTuOLv9DzTG6eYj5HBj8rTI+K34Av9a9ZtnzdV1XnY88zRwuzzE9s/y6AfPn9g84fyRPGsb9O+g8CQNAM5IwADQiCQNAI+aOhpayBr0WhtZJXs5fjrDNroizbDhurTxLa6PMob3WE6+PMs44S6E5DvTqiKNG3D2vco6Yr/qDV/Sb8KSHxQvfGXGtDpnnzO8qS6NfGjhGrd6esk0xfXWvTf9UhnkZh+TI5VpXiMW7xwuVMb0vmHlD77Uj88t3JJhdKH8wVx1/aaVVnoQBoBlJGAAakYQBoBE1YaDv/0Wc9dOsEY4y7vfOsiY4ypjdPMdaz7t9aOC1syN+a8SfiPj8iLN+m2N4Y5zwoEsizumls91Zfx137Ooo8x/XauX5+6iN0R6oha+76DtxJNo8dNnmKvOL52XYOcJv1JMwADQiCQNAI5IwADRi7migL2u6WcPLmm6t7phrA2etdRTLL1G7ekNL+eY5s921eZvzOlVq5Vl677qu2/nOeOEtEdeufa0+n8sHD9Uxx60r55juvG67K/uvZF7vVa79vT3r+wO18dPzd7wGPAkDQCOSMAA0IgkDQCNqwkBf/vM81+7dEXHWHbO4mXNFZ73tQSO06ZaID46wz51lXXOUGuLHIs4Ji/OY7404P+fTIo5xwrOv7zfhQMyr3CtLxhLFveuyFnf5rI1XltWt1sazTY+v7N919XHhOV563D4Eo4yPznZn7fujY56z8yQMAM1IwgDQiCQMAI1IwgDQSNOOWRZsYMP5SuX9nNg+O260+GfvHStow62V97MTSx7zOyrHW8l1WOVkDCuSHa+ys092SHtBxBdVto8FH7ZmR7Cu6/76k2W8Mzs5PSrinHQkO8llB6dRFsLIBRnyd17bvpZp8jOt5LveE3H+RvOYOSHIWmTD7x1/F0/CANCIJAwAjUjCANBI05rwDRFnmSHLL/l+lldq46g3rFsizguTEyfEYtWsofzR5aQFWQubhH/mZhuyRjxUf6vV5EaZ2ODO7hXx5yPOSTC6rutOG/Mc6yEnwsjvN2fOeGjEtb/FvI7P6G8yGzXh7saIsyacx8wb59HK+0NqN9uUdeba5B5p6PdXm3wjF4kYWpDjzvK7y3PmfbXr+td2f8Sj1NfDJNwiAOCkJAkDQCOSMAA0MrU04mDdqampNT/5eo8TzjLEpqkRMzluj3gSa8IrkZ+rVuvKmu+4rhl4LWuAWcPLWmZtXGgtHpLnvDnix41wjOVkfX5gnPChD5Txl+M3dl7ucEHEOQZ3X8S77qpxy8ibaY6nTrmoRG0xjRzb3HX932B+/7sjzkVHajXlUWrjKdt5U8SvqB9io94iAGDDk4QBoBFJGAAaaTpO+EQbGqqWFyDLDlk+qY1dzuGOtaFqbHAnYkx21mdrc+Dmjzh/5A+qHH/oGDW1uaZT1pBz3t+u67rrI86JBbKWWev0kdcta8hDd8McK5o14SvK8GWXlPGb/mlvEX/+p8qBx6e/Lo43MF76leUhuksujg2ujTivU/4+8jNlvXZozHatBpzXNm98tXps1q2zFj+KbFMeM8c6p9rNvev6fT5ybHK+PwJPwgDQiCQMAI1IwgDQyKYeJzwJbov41CatYFPJsaWp9k/rXBN5aLxsbf7hcQfd5zmyfpd1y6HXcp/amrW1+two44TzOlxVhku/VMbvic2f/fx4IWuIUfu8Leq/Xdd19/rN7yriC7qPF/FHfzY+aO1zZb01P+OXBvbJ7yI/R47JzfrruOsDr2Q94VoPp6z51tZlHmW+66x9Zz39gfVDeBIGgEYkYQBoRBIGgEbUhIGT06cizhrhkF+J+FDEWSvN+Yzz/Rx3/N6Bc+a43qzHZ/01t8967raIs645yuwRURvvHSPnrx533O9KasL5ufMYWRMemp/6zobGFWffiOMR59j1EeYW9yQMAI1IwgDQiCQMAI2cVHNHAwOGxh3XanLjrh+c5xjln/85p3XW9GrjhofGHt/Zd0X8+YFt8jpcFnFtzG3eYXO94KwhvnigDekTEefcz1mnrs3znONh8zMMvZbXOmvbT4t43DG8K1Gr6Wcbcmzz/oiH5o7Oenxe+xVkVE/CANCIJAwAjUjCANCImjCsxkpqnZNmlDbX5n6uyXrbuYNblWrrItfuXjluNH1hhDakvA45lrS2pm3WUm9cgzakPEdt+7zOQ3Nu5zYXRZxjbt8R8Y9FnDXmjId+X0cr22S9Nmu6eY68LlkjHlpTO+vxecza2OMBG/GWAQCbgiQMAI1IwgDQiCQMAI2csI5Z3xxYrCHr6uP29Th1xa2BNZL/jM0JJu5Z2X9oooy01hNnjCLPmR1OsvPOuH+8A4vX9+REGPcd8xw1+RmH2pSdu/KOmR2vsqNOTkKRx8vrODRpRV7bPEcuIpAdknIRiZQTbYwyWUfG+blygYffiPhFEWenqCH5feXnzDbkJCS132he+6HJXrLz10oWmgiehAGgEUkYABqRhAGgkRNWEx63ZAQbUq0GnIZqSvla1h1rk1jUPGiEbbLeljW7oQkdlpM3gJy8Y8hQbXIctXp71hBzIoauqy8SkXXEoUn/7ywn9zi/sn3X9Wub+f1nnXLchQzyOgwtZn965Rz5uV8d8U9EnJO1ZD1+aKKMVOu3kGrHzN/b0HeZx1jtb7TzJAwAzUjCANCIJAwAjZywmnAOZRs6udUkOOkM1ZRq40Lz/ZzYfg3GLlaNWwurTZY/JGt84y5cULNrhP1r9dWsldZuYvmZ8joMjROuLRKQ+9R+Hynrsyupc+Z3EzX/j0Ut9fzawhZD9d3aWOU8Ru39PEdtgYeuW5ck5UkYABqRhAGgEUkYABo5YWXYoWk48+RnnIiGwCQZqjvVxjNmbau2eH0aZYxuyj/WHJtak3XKoRtCymtTqwmPWwvPsak5Hrfr6uNC85zjToiQ3+Uo48bz2tVqwrVrvS3iUcbo1uaOjjrz+dmmPEfe/P92hDbkeOZsU/5m8/2HRnx9xEPfZX5fQ2Oqx+RJGAAakYQBoBFJGAAaOWE14aHhb3nyg2Mec7XT2cK6G2W94FSrK9ZqXynrb6OM0a2ds6b2GWp1za4bv8ZbO2deh6znDdVORxk7eme1+uy4x+u6frtrc4dnXKtb7ot4qI9BvlZbNznr67dEfEPEeyIeupnXzlmrZed1yb+DbMPQ7yn7EVhPGAA2LkkYABqRhAGgkabTNed/4Q/NL72cWglIjZjm8p+5WSMe+gusrVlbqzum2pjMIXmO2hzKqVYrOzviobHLtWPUxp7Wtr9xhP1rbcjx0ll/rbVxlJpifhf5+8j4S5X9c53cbPMo6+imPMbVle1TzuNcW5e568YfR15bG3oUtfr8CngSBoBGJGEAaEQSBoBGJmoJ33GnXR33eGrENDdKHas2trS2fc1K5o4eV36uWl1zyCh1weXOkW3I+t3OiG8eOGZtDdtxv5tc27e21m/XjT8WNb/f3D/XD873h9qU30Veq1dUzvHbEde+/6Fx4zn2eNwbevZrWEk9dyVrLVd4EgaARiRhAGhEEgaARk7g3NH9wsYpq/wP9iwb1KZIXRp4bWpVLYAx3asSnwij1MLyzvDAVZ7zTyPOmuLQH2/t9pC3lENj7p+fcfcI58jxrOPWGWt17qE657gTImStO/fPmnFe+/yMQ+d4faVNF0RcG9ue1zHns+66fjtPqbQhJ54YWi96OeOu071CnoQBoBFJGAAakYQBoBFJGAAaWbeOWR9YqlfBxx1rXetnkbX9WketrhvurLUcHbnY8B7U4Jzf3+Cc6S8izhvK0KQYuU12rMp98o6aN6WVzCBU65iVbahtn4vZ5yIUQ9fhjyvHfFjEtUlI8uac1/mhA/uMu2BDreNWfjej/B5q39e4E6t0noQBoBlJGAAakYQBoJGJWsBhXF9fh2OOO2d8yrKCGjJMiCdU3v/swGtZA9wTcd5B8wZwz4gvrew/VHPM7jVZd8xzPiLiXRF/MOKc5OSqgTZ8IeKcvOX8iLPOnDfWvRGPkonWevGEbGMamjAkr2V+X1lnHoEnYQBoRBIGgEYkYQBoZN1qwlvi0LMr+A/9cffYFvFK1mw+GPEoY42XM8pnUDeGCZBjXdfD607AOWrypnRTxFcP7PPGiC+O+LQx23BZxFkzHrpxnhtxLlRRG6ObN/P/GHFmw1yEouv69fRxxy4P8CQMAI1IwgDQiCQMAI2sW014ISqycyv5z/Jo3szYs03XZavGrQHXpicd5VNn+WMltew7W/urBLSS89tnH5J8f/HjZXzlfy7jZ7899o/a51TWSkdwW+X94xFv/8nxz1F1RxkeOlDGcxGf/ZbY/14jnCNvrnnzri+Z0ONJGAAakYQBoBFJGAAamVpaWhppSd2pqfFGs46ynnDN9qi41mrCOU54FOtdE05Dw99WWwMe14mYMNzYZxjfKDfjXKr3isv2F/HM75V3madcvKOIP3lDuf9DXlTGuwbm2M775OduL+OZmCO7v2zy8gsp55TLQ/fE2V4r5so23lrmnPn5cvvFhfKc58yVA4/n4sb4mYFk8KOnDTRslTwJA0AjkjAANCIJA0AjasK9c65OrUbcoia8oReNXoa6M5PuyK1lvL0yFvUP/3Su99q+95Z3lV3T5X3wrJ8ut98Vc2Dn3//0LWU8c9rybRqyL+qxc5WV2PO+N93l5yyrwjO9GnLX3RTjgA8eK6/DzJbyLPML5d197kiZk3bNLJ9P5nuTU3fdl7dOxzZxHY6Vg5Ffdf9cgLrPkzAANCIJA0AjkjAANLJZy4XN5BypLWzWuaPVgDevoY4pG+H77s3b/A9l/KFYP/ixF5Xx+/+gjGcX+rXV57y0jLdFzbd2z+n1OTmtDHPccb8a2z/GXG+rHOmb25f12TOi3jodvWkODn2qGPe7I9owf6x8f2a+rAFnXXp/dAiaXdwRW/SvxBl5c40Meth6wgCwcUjCANCIJAwAjaxbTfjJU1vX69D/6sbRhjifdIZqOutN5wLGMcpvdLV9Gw59pYxP/45VHrDruo9dU8Z/994y3hGNfuzjy/hDbyjj739eGW+PtX27ruu2xLzMh6K+umVw9oG7lvXdld0vxpvdINeXP9gduIst/9n8Hf3i6lyOLY5rvRjnWKis5j47V84s8cUje4v4nO3l3NJd13XTUWc+Ope18P447xpPwgDQiCQMAI1IwgDQiCQMAI1s6P40u2NRia9WOmqtYBw1sA6y09VQ56DbIp6LiTD2XV/Gf/fRMj7r9DK+LvrQPP1VZXzTX/TbcODGMj4e/ZGe8qIy3hETaaQffVq8EI9Bo3Q1rXWJ2giT9WRHrZSdrNbD4kw5Tcl0LM5w0+EbevvMRB+4uelo55bxu7l5EgaARiRhAGhEEgaARjZ0TTh9+1T7Kd//MerSp9zFdutpI9SE2NyOxWL2WyuL2S98vv/aJz9QxvNRfttxRhk/8RllvPNRZfyxmFjj9U8v43PO77fhMTGZxux9y7h2x1mIhQ2m77Z92e1HmephW0zOkdXTWjU1q5ajVF/7CzZsfltm+5Og9K5V/CgX5tSEAWDDkIQBoBFJGAAa2VQ14UlwnwmoS3/zBCxscSJ+OO2v5MnrttvL+Lq3lvG+j5Xxjt1l/PAYD/uZGG+7P+q9535nvw2PPa+M584q460PiB0qjxTnP6uMHx6LKywOlGtnK8esVQCnu/KguQhBLmY/PUKPjnFrwLX9j45U7z0+5lkYlSdhAGhEEgaARiRhAGhETXgTuscE1KXvWIO6dB6h/afqyzYejjimL16Rmz9Vxp+J8a5HD5XxmVGf3fG9ZfzX/7OMZ5Yfutp1XdedFzXeJ11S3+fOzn5wvPCswc2WtXXM7XPu6ay2zsSY3yFZP83qaa2Cm/PVz3azYx1/SO2mvf6zLrOWPAkDQCOSMAA0IgkDQCNqwqyLuzWoSy+tQR0664hZX/tyzHG89+oyPnhzGV/44jLed20Zz8cauF3XdTuiRrtjTxk/NOYzPjPqrafG8fZ+vIwveWMZT2KtfSXyu0tZb12L9cVHmev5zmo15KF6bq3Gezg+2XzlLDsqx+PE8iQMAI1IwgDQiCQMAI2oCbNpTEUdOmvEWTN8/5X9YxyMGu/MwTI+J8bLPuXiMj41xp6+77fK+MxdZbznnf02rLVzH73+5zhZrLaOnOPIR7E/znq8WiUu53mejdv8QSOJJ4onYQBoRBIGgEYkYQBoRE2YTStrxGvifRG/cPnN12Ls8slg6CrlGFw3q5WZq9SA87pOr6DyfbyyT+27m7+jjA/PD+1RzrvdLZSf6/h8f6buog2L5S/q4Fx5jsVRJu5eB56EAaARSRgAGpGEAaARZRZYR+tSlx7TJNSlswVZvRua7ThvTscjzhLeTGV/2smq9Nwd5bdzbL632nP/GAsxR/Z8HrXcZzF+ZTO94nf8CkeoCec5FxaP1Xeq8CQMAI1IwgDQiCQMAI1IwgDQiL4LsMm16Bz2zegMln1eaovbd52OWP9iZpWfrL7gw8nhyHxM/xKXdTp6bh2ZG1puI36FcYyVXGlPwgDQiCQMAI1IwgDQyGYtowAN3WMCJim5dQImKVkPWyq37QU14JWZjus2PTR7x3iLYYzCkzAANCIJA0AjkjAANKImDGxK95qAuvTXNmldmrXjSRgAGpGEAaARSRgAGplamoQVvwHgJORJGAAakYQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAakYQBoBFJGAAa+f/WNVkeIZK6VwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from config import Config\n",
    "from model import CNN_LSTM\n",
    "from transforms import get_transforms\n",
    "\n",
    "# Load model function\n",
    "def load_model():\n",
    "    model = CNN_LSTM(\n",
    "        num_classes=Config.num_classes,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        hidden_dim=Config.hidden_dim,\n",
    "        lstm_layers=Config.lstm_layers,\n",
    "        dropout=Config.dropout\n",
    "    ).to(Config.device)\n",
    "\n",
    "    checkpoint = torch.load(Config.model_save_path, map_location=Config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Preprocess video function\n",
    "def preprocess_video(video_path, transform):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    if total_frames < Config.frames_per_clip:\n",
    "        frames += [frames[-1]] * (Config.frames_per_clip - total_frames)\n",
    "    else:\n",
    "        step = total_frames // Config.frames_per_clip\n",
    "        frames = frames[::step][:Config.frames_per_clip]\n",
    "\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    frames = torch.stack(frames, dim=0)\n",
    "    return frames\n",
    "\n",
    "# Display image\n",
    "def show_image(image, display_size=(6, 6)):\n",
    "    plt.figure(figsize=display_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Predict a single video\n",
    "def predict_single_video(video_path, model, transform):\n",
    "    frames = preprocess_video(video_path, transform)\n",
    "    frames_for_display = frames[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    frames = frames.unsqueeze(0).to(Config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "        pred = outputs.argmax(dim=1).item()\n",
    "\n",
    "    predicted_class = Config.class_names[pred]\n",
    "\n",
    "    print(f\"Video: {video_path}\")\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "\n",
    "    print(\"\\n Displaying first frame from the video:\")\n",
    "    show_image(frames_for_display)\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Predict all videos in a folder\n",
    "def predict_videos_in_folder(folder_path):\n",
    "    model = load_model()\n",
    "    transform = get_transforms(Config.img_size)\n",
    "\n",
    "    video_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.avi', '.mp4', '.mov'))]\n",
    "\n",
    "    print(f\"\\n Found {len(video_files)} videos in '{folder_path}'\\n\")\n",
    "\n",
    "    for video_path in video_files:\n",
    "        try:\n",
    "            predict_single_video(video_path, model, transform)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {video_path}: {e}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = r\"data/UCF101/Basketball/v_Basketball_g01_c01.avi\"  # single video\n",
    "    model = load_model()\n",
    "    transform = get_transforms(Config.img_size)\n",
    "    predict_single_video(video_path, model, transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2a055",
   "metadata": {},
   "source": [
    "display video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c4d3cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from config import Config\n",
    "from model import CNN_LSTM\n",
    "from transforms import get_transforms\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads the trained model with the saved state.\n",
    "    \"\"\"\n",
    "    model = CNN_LSTM(\n",
    "        num_classes=Config.num_classes,\n",
    "        sequence_length=Config.frames_per_clip,\n",
    "        hidden_dim=Config.hidden_dim,\n",
    "        lstm_layers=Config.lstm_layers,\n",
    "        dropout=Config.dropout\n",
    "    ).to(Config.device)\n",
    "\n",
    "    checkpoint = torch.load(Config.model_save_path, map_location=Config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_video(video_path, transform):\n",
    "    \"\"\"\n",
    "    Preprocesses a video by extracting frames and applying necessary transformations.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    total_frames = len(frames)\n",
    "    if total_frames < Config.frames_per_clip:\n",
    "        frames += [frames[-1]] * (Config.frames_per_clip - total_frames)\n",
    "    else:\n",
    "        step = total_frames // Config.frames_per_clip\n",
    "        frames = frames[::step][:Config.frames_per_clip]\n",
    "\n",
    "    frames = [transform(frame) for frame in frames]\n",
    "    frames = torch.stack(frames, dim=0)  # (sequence_length, 3, img_size, img_size)\n",
    "    return frames\n",
    "\n",
    "def predict_single_video(video_path, model, transform):\n",
    "    \"\"\"\n",
    "    Predicts the activity for a single video and displays it along with the prediction.\n",
    "    \"\"\"\n",
    "    frames = preprocess_video(video_path, transform)\n",
    "    frames = frames.unsqueeze(0).to(Config.device)  # add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "\n",
    "    predicted_class = Config.class_names[pred.item()]\n",
    "    print(f\"\\n{video_path} ➔ Predicted Class: {predicted_class}\\n\")\n",
    "\n",
    "    # PLAY the original video with the prediction\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"Playing the video... Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (640, 480))  # Optional: resize for display\n",
    "        cv2.putText(frame, f'Prediction: {predicted_class}', (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Video Prediction', frame)\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your video path\n",
    "    video_path = r\"data/UCF101/Basketball/v_Basketball_g01_c01.avi\"  \n",
    "    model = load_model()\n",
    "    transform = get_transforms(Config.img_size)\n",
    "    predict_single_video(video_path, model, transform)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
